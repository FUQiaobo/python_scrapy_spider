#新闻链接详情页
news = 'http://news.sina.com.cn/o/2017-10-04/doc-ifymkwwk8393020.shtml'
import re
import json
# 创建获取新闻评论数函式
def getCommentCounts(newsurl):    
    #正则表达式筛选内容ID
    m = re.search('doc-i(.+).shtml',newsurl)
    newsid = m.group(1)
    comments = requests.get(commentURL.format(newsid))
    jd = json.loads(comments.text.strip('var data='))
    return jd['result']['count']['total']
import requests
from bs4 import BeautifulSoup
import lxml
#创建新闻详情内容dic
def getNewsDetail(newsurl):
    #创建result字典，将内容存到字典里
    result = {}
    #获取页面
    res = requests.get(newsurl)
    #编码为utf-8
    res.encoding = 'utf-8'
    #放进美丽汤
    soup = BeautifulSoup(res.text,'lxml')
    #获取标题
    result['title'] = soup.select('#artibodyTitle')[0].text
    #获取新闻链接
    result['newssource'] = soup.select('.time-source span a')[0].text
    #获取时间
    result['timesource'] = soup1.select('.time-source')[0].contents[0]
    #获取文章内容
    result['artitle'] = ' '.join(p.text.strip() for p in soup.select('#artibody p')[:-1])
    #获取评论数
    result['comments'] = getCommentCounts(newsurl)
    #反回结果
    return result
    #测试
getNewsDetail('http://news.sina.com.cn/o/2017-10-04/doc-ifymkwwk8393020.shtml')
 
# 函式获取每则新闻链接
url = 'http://api.roll.news.sina.com.cn/zt_list?channel=news&cat_1=gnxw&cat_2==gdxw1||=gatxw||=zs-pl||=mtjj&level==1||=2&\
show_ext=1&show_all=1&show_num=22&tag=1&format=json&page=2&callback=newsloadercallback&_=1507106270438'
def parseListLinks(url):  
    #创建列表
    newsdetails = []
    #获取监控器里面JS链接
    res = requests.get(url)
    #json格式转化
    jd = json.loads(res.text.lstrip('  newsloadercallback(').rstrip(');'))
    #将链接填充到newsdetails里面
    for ent in jd['result']['data']:
        newsdetails.append(getNewsDetail(ent['url']))
    return newsdetails
parseListLinks(url)

 
# 抓取每个分页数做for循环
url = 'http://api.roll.news.sina.com.cn/zt_list?channel=news&cat_1=gnxw&cat_2==gdxw1||=gatxw||=zs-pl||=mtjj&level==1||=2&\
show_ext=1&show_all=1&show_num=22&tag=1&format=json&page={}&callback=newsloadercallback&_=1507106270438'
news_total = []
for i in range(1,10):
    newsurl = url.format(i)
    newsary = parseListLinks(newsurl)
    news_total.extend(newsary)
    
    
#pandas工具   
import pandas
df = pandas.DataFrame(news_total)
df.to_excel('news.xlsx')
# 存到sqlite3里面
import sqlite3
with sqlite3.connect('news.sqlite') as db:
    df.to_sql('news1',con = db)
